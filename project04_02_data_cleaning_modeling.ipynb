{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 4: Data Cleaning & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import All Libraries & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:59:53.341729Z",
     "start_time": "2019-11-14T19:59:50.542948Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "\n",
    "import html.parser\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "\n",
    "from   nltk.tokenize      import WordPunctTokenizer,word_tokenize\n",
    "from   nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from   nltk.corpus        import stopwords\n",
    "from   nltk.stem          import PorterStemmer\n",
    "\n",
    "from   sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from   sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from   sklearn.model_selection         import train_test_split\n",
    "from   sklearn.decomposition           import TruncatedSVD\n",
    "from   sklearn.decomposition           import NMF\n",
    "from   sklearn.cluster                 import KMeans\n",
    "from   sklearn.metrics.pairwise        import cosine_similarity\n",
    "\n",
    "from   sklearn.decomposition           import PCA\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:59:53.906552Z",
     "start_time": "2019-11-14T19:59:53.344021Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set global parameters\n",
    "SEED                    = 42     # Random seed\n",
    "sample_test_size        = 0.05    # 5% of sample size for testing\n",
    "cv_value                = 5     # Cross validation number of folds\n",
    "\n",
    "# Load stop words and punctuations\n",
    "stop_words          = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "exclude_punctuation = set(string.punctuation) \n",
    "lemmatizer          = WordNetLemmatizer()\n",
    "stemmer             = PorterStemmer()\n",
    "\n",
    "#sp                  = spacy.load('en', disable=['parser', 'ner'])\n",
    "sp                  = spacy.load('en')\n",
    "\n",
    "# Tokenizer object\n",
    "tok                 = WordPunctTokenizer()\n",
    "\n",
    "# This is a handy dictionary taken from a git python of tthustla. As negations are important\n",
    "# for sentiment analysis and can change the overall picure\n",
    "# Not used in the end but nice to have for later\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "\n",
    "# this value is to be used later in the function \n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:59:53.912272Z",
     "start_time": "2019-11-14T19:59:53.908629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extend stop words\n",
    "stop_words.extend(['flight', 'fly','flights','flies','sent','just','don','flying',\n",
    "                    'got','plane','getting','get','thanks','delta','going','southwest',\n",
    "                    'know','sure','delayed','airline','olivia','want','look','did',\n",
    "                    'did','time','let','airline','really','time','need','new','hey','united',\n",
    "                    'doe','people','doesn','isn','american','didn','doe','man','jetblue','make',\n",
    "                    'tayler','englishboston','airline','jet blue','yeah','john','dont','doing'\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:04:36.725181Z",
     "start_time": "2019-11-14T20:04:36.700141Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's only focus on Delta airlines\n",
    "\n",
    "file_path = './data/'\n",
    "#file_name = 'pulled_airline_tweets_delta_no_RT_main.csv'\n",
    "file_name = 'pulled_airline_tweets_delta_no_RT2_subset_w_secs.csv'\n",
    "\n",
    "csv_file  = file_path+file_name\n",
    "\n",
    "df_all = pd.read_csv(csv_file,\n",
    "                 encoding ='utf-8-sig',   #utf-16-le, utf-8-sig\n",
    "                 header   = None,\n",
    "                 names  =['airline', 'date', 'raw tweet'])   # Add header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T19:59:55.104007Z",
     "start_time": "2019-11-14T19:59:55.100747Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Let's load all files and then combine all\n",
    "fn_jb = './data/pulled_airline_tweets_jetblue_no_RT.csv'\n",
    "fn_sw = './data/pulled_airline_tweets_southwest_no_RT.csv'\n",
    "fn_un = './data/pulled_airline_tweets_united_no_RT.csv'\n",
    "fn_am = './data/pulled_airline_tweets_american_no_RT.csv'\n",
    "fn_de = './data/pulled_airline_tweets_delta_no_RT.csv'\n",
    "\n",
    "\n",
    "df_jb = pd.read_csv(fn_jb,\n",
    "                    encoding ='utf-8-sig',   #utf-16-le, utf-8-sig\n",
    "                    header   = None,\n",
    "                    names  =['airline', 'date', 'raw tweet'])   # Add header\n",
    "\n",
    "df_sw = pd.read_csv(fn_sw,\n",
    "                    encoding ='utf-8-sig',   #utf-16-le, utf-8-sig\n",
    "                    header   = None,\n",
    "                    names  =['airline', 'date', 'raw tweet'])   # Add header\n",
    "\n",
    "df_un = pd.read_csv(fn_un,\n",
    "                    encoding ='utf-8-sig',   #utf-16-le, utf-8-sig\n",
    "                    header   = None,\n",
    "                    names  =['airline', 'date', 'raw tweet'])   # Add header\n",
    "\n",
    "df_am = pd.read_csv(fn_am,\n",
    "                    encoding ='utf-8-sig',   #utf-16-le, utf-8-sig\n",
    "                    header   = None,\n",
    "                    names  =['airline', 'date', 'raw tweet'])   # Add header\n",
    "\n",
    "df_de = pd.read_csv(fn_de,\n",
    "                    encoding ='utf-8-sig',   #utf-16-le, utf-8-sig\n",
    "                    header   = None,\n",
    "                    names  =['airline', 'date', 'raw tweet'])   # Add header\n",
    "\n",
    "# Now test appending\n",
    "df_all = df_jb.append(df_sw, ignore_index=True)\n",
    "df_all = df_all.append(df_un, ignore_index=True)\n",
    "df_all = df_all.append(df_am, ignore_index=True)\n",
    "df_all = df_all.append(df_de, ignore_index=True)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:04:40.532427Z",
     "start_time": "2019-11-14T20:04:40.524622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>date</th>\n",
       "      <th>raw tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delta</td>\n",
       "      <td>11/10/19 23:59:48</td>\n",
       "      <td>b'Hey @Delta, when do you plan to change the S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delta</td>\n",
       "      <td>11/10/19 23:56:57</td>\n",
       "      <td>b'@Delta - Annoying and disappointing. Getting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Delta</td>\n",
       "      <td>11/10/19 23:56:51</td>\n",
       "      <td>b'@Delta Why is DL 2167 late ?'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delta</td>\n",
       "      <td>11/10/19 23:49:50</td>\n",
       "      <td>b\"@MLGPuckett @Delta @dallascowboys @Vikings @...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delta</td>\n",
       "      <td>11/10/19 23:48:07</td>\n",
       "      <td>b'@solitarybrother @Delta Nice'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline               date  \\\n",
       "0   Delta  11/10/19 23:59:48   \n",
       "1   Delta  11/10/19 23:56:57   \n",
       "2   Delta  11/10/19 23:56:51   \n",
       "3   Delta  11/10/19 23:49:50   \n",
       "4   Delta  11/10/19 23:48:07   \n",
       "\n",
       "                                           raw tweet  \n",
       "0  b'Hey @Delta, when do you plan to change the S...  \n",
       "1  b'@Delta - Annoying and disappointing. Getting...  \n",
       "2                    b'@Delta Why is DL 2167 late ?'  \n",
       "3  b\"@MLGPuckett @Delta @dallascowboys @Vikings @...  \n",
       "4                    b'@solitarybrother @Delta Nice'  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:04:41.296178Z",
     "start_time": "2019-11-14T20:04:41.292082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5159, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:09.878410Z",
     "start_time": "2019-11-14T20:05:09.871046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5159 entries, 0 to 5158\n",
      "Data columns (total 3 columns):\n",
      "airline      5159 non-null object\n",
      "date         5159 non-null object\n",
      "raw tweet    5159 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 121.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:10.627277Z",
     "start_time": "2019-11-14T20:05:10.624804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Grab the text only\n",
    "df_text_raw = df_all['raw tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:10.799652Z",
     "start_time": "2019-11-14T20:05:10.794710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'Hey @Delta, when do you plan to change the S...\n",
       "1    b'@Delta - Annoying and disappointing. Getting...\n",
       "2                      b'@Delta Why is DL 2167 late ?'\n",
       "3    b\"@MLGPuckett @Delta @dallascowboys @Vikings @...\n",
       "4                      b'@solitarybrother @Delta Nice'\n",
       "Name: raw tweet, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look\n",
    "df_text_raw[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning, Tokenization, Lemmatization .etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:11.339346Z",
     "start_time": "2019-11-14T20:05:11.336745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@jeffwilcox @Delta Never seen it before - still not used to this \\xe2\\x80\\x98tip for everything\\xe2\\x80\\x99 culture.\\n\\nI have status so can use the priority lines anyway, so will avoid it in the future.'\n"
     ]
    }
   ],
   "source": [
    "# Remove b' from the beginning of the text \n",
    "print(df_text_raw[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:11.537545Z",
     "start_time": "2019-11-14T20:05:11.535034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@jeffwilcox @Delta Never seen it before - still not used to this \\xe2\\x80\\x98tip for everything\\xe2\\x80\\x99 culture.\\n\\nI have status so can use the priority lines anyway, so will avoid it in the future.'\n"
     ]
    }
   ],
   "source": [
    "print(df_text_raw[11].replace(\"b'\",''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:11.689105Z",
     "start_time": "2019-11-14T20:05:11.686403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Delta seems like every Sunday lately when I fly and want to watch NFL games the satellite is down and the WiFi isn\\xe2\\x80\\x99t working. #Fail DL1224 https://t.co/GjDY4vGMe8'\n"
     ]
    }
   ],
   "source": [
    "# Remove \\n\n",
    "print(re.sub(r'\\\\n','',df_text_raw[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:11.851689Z",
     "start_time": "2019-11-14T20:05:11.848994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Delta seems like every Sunday lately when I fly and want to watch NFL games the satellite is down and the WiFi isn\\xe2\\x80\\x99t working. #Fail DL1224 '\n"
     ]
    }
   ],
   "source": [
    "# Remove https: or ulrs\n",
    "print(re.sub('https?://[A-Za-z0-9./]+','',df_text_raw[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:12.000976Z",
     "start_time": "2019-11-14T20:05:11.998165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' seems like every Sunday lately when I fly and want to watch NFL games the satellite is down and the WiFi isn\\xe2\\x80\\x99t working. #Fail DL1224 https://t.co/GjDY4vGMe8'\n"
     ]
    }
   ],
   "source": [
    "# Remove @ mention\n",
    "print(re.sub('@[A-Za-z0-9_]+','',df_text_raw[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:12.272962Z",
     "start_time": "2019-11-14T20:05:12.266445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Cab98Nate @Delta I\\xe2\\x80\\x99m not one either but can\\xe2\\x80\\x99t They avoid certain altitudes and the direction of where they fly to avoid severe turbulence? Was the worst. Didn\\xe2\\x80\\x99t say they could control it. Other airlines have better pilots? \\xf0\\x9f\\xa4\\xb7\\xf0\\x9f\\x8f\\xbb\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f'\n"
     ]
    }
   ],
   "source": [
    "# Replace (hashtag) #word with word\n",
    "print(re.sub(r'#([^\\s]+)', r'\\1', df_text_raw[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:12.415571Z",
     "start_time": "2019-11-14T20:05:12.412801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Cab98Nate @Delta I\\xe2\\x80\\x99m not one either but can\\xe2\\x80\\x99t They avoid certain altitudes and the direction of where they fly to avoid severe turbulence? Was the worst. Didn\\xe2\\x80\\x99t say they could control it. Other airlines have better pilots? \\xf0\\x9f\\xa4\\xb7\\xf0\\x9f\\x8f\\xbb\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f'\n"
     ]
    }
   ],
   "source": [
    "# HTML decoding\n",
    "print(html.unescape(df_text_raw[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:12.573592Z",
     "start_time": "2019-11-14T20:05:12.571157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Cab98Nate @Delta I\\xe2\\x80\\x99m not one either but can\\xe2\\x80\\x99t They avoid certain altitudes and the direction of where they fly to avoid severe turbulence? Was the worst. Didn\\xe2\\x80\\x99t say they could control it. Other airlines have better pilots? \\xf0\\x9f\\xa4\\xb7\\xf0\\x9f\\x8f\\xbb\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f'\n"
     ]
    }
   ],
   "source": [
    "# Remove annoying BOM characters\n",
    "print(df_text_raw[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:12.730071Z",
     "start_time": "2019-11-14T20:05:12.727118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@Cab98Nate @Delta Im not one either but cant They avoid certain altitudes and the direction of where they fly to avoid severe turbulence? Was the worst. Didnt say they could control it. Other airlines have better pilots? '\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\\\x[a-z0-9A-Z]{2}','', df_text_raw[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:12.887462Z",
     "start_time": "2019-11-14T20:05:12.884259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bCab98Nate Delta Ixe2x80x99m not one either but canxe2x80x99t They avoid certain altitudes and the direction of where they fly to avoid severe turbulence Was the worst Didnxe2x80x99t say they could control it Other airlines have better pilots xf0x9fxa4xb7xf0x9fx8fxbbxe2x80x8dxe2x99x82xefxb8x8f\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation symbols\n",
    "text = df_text_raw[16]\n",
    "print(''.join(ch for ch in text if ch not in exclude_punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:13.148155Z",
     "start_time": "2019-11-14T20:05:13.144964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'@cab98nate @delta i\\xe2\\x80\\x99m not one either but can\\xe2\\x80\\x99t they avoid certain altitudes and the direction of where they fly to avoid severe turbulence? was the worst. didn\\xe2\\x80\\x99t say they could control it. other airlines have better pilots? \\xf0\\x9f\\xa4\\xb7\\xf0\\x9f\\x8f\\xbb\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f'\n"
     ]
    }
   ],
   "source": [
    "# All lower case \n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:13.303551Z",
     "start_time": "2019-11-14T20:05:13.301130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"@MLGPuckett @Delta @dallascowboys @Vikings @hastr0 @CoreyDunn @PUCKETT111 I'm always rooting for an NFC north rival win... enjoy Jerry's World and get the W! #GoPackGo\"\n"
     ]
    }
   ],
   "source": [
    "# Numbers\n",
    "text = df_text_raw[3]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:13.461107Z",
     "start_time": "2019-11-14T20:05:13.458405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b  MLGPuckett  Delta  dallascowboys  Vikings  hastr   CoreyDunn  PUCKETT    I m always rooting for an NFC north rival win    enjoy Jerry s World and get the W   GoPackGo \n"
     ]
    }
   ],
   "source": [
    "# Remove numbers\n",
    "print(re.sub(\"[^a-zA-Z]\", \" \", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:13.617612Z",
     "start_time": "2019-11-14T20:05:13.614780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', '\"@', 'MLGPuckett', '@', 'Delta', '@', 'dallascowboys', '@', 'Vikings', '@', 'hastr0', '@', 'CoreyDunn', '@', 'PUCKETT111', 'I', \"'\", 'm', 'always', 'rooting', 'for', 'an', 'NFC', 'north', 'rival', 'win', '...', 'enjoy', 'Jerry', \"'\", 's', 'World', 'and', 'get', 'the', 'W', '!', '#', 'GoPackGo', '\"']\n"
     ]
    }
   ],
   "source": [
    "print(tok.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:14.048480Z",
     "start_time": "2019-11-14T20:05:14.044361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b \"@ MLGPuckett @ Delta @ dallascowboys @ Vikings @ hastr0 @ CoreyDunn @ PUCKETT111 I ' m rooting NFC north rival win ... enjoy Jerry ' s World W ! # GoPackGo \"\n"
     ]
    }
   ],
   "source": [
    "# Remove stop-words\n",
    "print(' '.join(i for i in tok.tokenize(text) if (i.strip() not in stop_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:15.465994Z",
     "start_time": "2019-11-14T20:05:14.199764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b '' @ MLGPuckett @ Delta @ dallascowboys @ Vikings @ hastr0 @ CoreyDunn @ PUCKETT111 I 'm always rooting for an NFC north rival win ... enjoy Jerry 's World and get the W ! # GoPackGo ''\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization via nltk\n",
    "#print(' '.join(lemmatizer.lemmatize(word) for word in text.split()))\n",
    "\n",
    "print(' '.join(lemmatizer.lemmatize(word) for word in word_tokenize(text)))\n",
    "# The lemmatizer is installed on my /Users/phoitack/nltk_data\n",
    "#print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:15.482129Z",
     "start_time": "2019-11-14T20:05:15.467862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"@MLGPuckett @delta @dallascowboys @Vikings @hastr0 @CoreyDunn @PUCKETT111 -PRON- be always root for an NFC north rival win ... enjoy Jerry 's World and get the W ! # gopackgo \"\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization via spacy\n",
    "# I would not use this for now\n",
    "doc = sp(text)\n",
    "\n",
    "print(' '.join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:15.489155Z",
     "start_time": "2019-11-14T20:05:15.484721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw: \n",
      "\n",
      "b\"@MLGPuckett @Delta @dallascowboys @Vikings @hastr0 @CoreyDunn @PUCKETT111 I'm always rooting for an NFC north rival win... enjoy Jerry's World and get the W! #GoPackGo\"\n",
      "\n",
      "Stemmed: \n",
      "\n",
      "b '' @ mlgpuckett @ delta @ dallascowboy @ vike @ hastr0 @ coreydunn @ puckett111 I 'm alway root for an nfc north rival win ... enjoy jerri 's world and get the W ! # gopackgo ''\n"
     ]
    }
   ],
   "source": [
    "# Stem via NLTK\n",
    "#print(' '.join([token.lemma_ for token in doc]))\n",
    "input_str = word_tokenize(text)\n",
    "\n",
    "print('\\nRaw: \\n')\n",
    "print(text)\n",
    "print('\\nStemmed: \\n')\n",
    "print(' '.join(stemmer.stem(word) for word in word_tokenize(text)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:15.498019Z",
     "start_time": "2019-11-14T20:05:15.491381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Tweet:\n",
      " b\"@MeatheadMilitia @SouthwestAir @Delta Ah I see. Well. Either way the savings are worth it I think. That's a whole new video card. \\xf0\\x9f\\x98\\x84\"\n",
      "\n",
      "Cleaned Tweet:\n",
      " ah i see  well  either way the savings are worth it i think  that s a whole new video card\n"
     ]
    }
   ],
   "source": [
    "# Put it all in a function\n",
    "def clean_tweets(text):\n",
    "    \n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove the b' at the beginning\n",
    "    text = text.replace(\"b'\",'')\n",
    "    \n",
    "    # Remove \\n\n",
    "    text = re.sub(r'\\\\n','',text)\n",
    "    \n",
    "    # Handle negation properly\n",
    "    #text = neg_pattern.sub(lambda x: negations_dic[x.group()], text)\n",
    "    \n",
    "    # Remove https\n",
    "    text = re.sub('https?://[A-Za-z0-9./]+','', text)\n",
    "    \n",
    "    # Remove @ mention\n",
    "    text = re.sub('@[A-Za-z0-9_]+','', text)\n",
    "    \n",
    "    # Replace (hashtag) #word with word\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    \n",
    "    # html unescape\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove BOM characters. This took me all night.\n",
    "    text = re.sub(r'\\\\x[a-z0-9A-Z]{2}', ' ', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    # trim\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    # Remove punctuation and symbols\n",
    "    text = ''.join(ch for ch in text if ch not in exclude_punctuation)\n",
    "    \n",
    "    # This removes the b' at the beginning if it did not catch it the first time\n",
    "    # due to b''\n",
    "    if text[0] == 'b':\n",
    "        text = text[1:]\n",
    "    else:\n",
    "        text = text\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return (text)\n",
    "\n",
    "\n",
    "text_new = clean_tweets(df_text_raw[99])\n",
    "\n",
    "print('Raw Tweet:\\n',df_text_raw[99])\n",
    "print('\\nCleaned Tweet:\\n',text_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:15.503246Z",
     "start_time": "2019-11-14T20:05:15.499726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah way savings worth think s video card\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "\n",
    "    tokenized_text = tok.tokenize(text)\n",
    "    text = ' '.join(i for i in tokenized_text if (i.strip() not in stop_words))\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "text_new2 = remove_stopwords(text_new)\n",
    "\n",
    "print(text_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:15.507950Z",
     "start_time": "2019-11-14T20:05:15.504843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah way saving worth think s video card\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(text):\n",
    "    \n",
    "    # lemmatize: replace word with root of the word.\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in word_tokenize(text))\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "text_new3 = lemmatize(text_new2)\n",
    "\n",
    "print(text_new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:18.390904Z",
     "start_time": "2019-11-14T20:05:18.386687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah way save worth think s video card\n"
     ]
    }
   ],
   "source": [
    "def stem(text):\n",
    "    \n",
    "    text = ' '.join(stemmer.stem(word) for word in word_tokenize(text))\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "text_new3 = stem(text_new2)\n",
    "\n",
    "print(text_new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:18.664697Z",
     "start_time": "2019-11-14T20:05:18.541622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ah way save worth think s video card\n"
     ]
    }
   ],
   "source": [
    "def remove_non_english(text):\n",
    "    \n",
    "    words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    text = ' '.join(w for w in word_tokenize(text) \\\n",
    "                       if w.lower() in words or not w.isalpha())\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "print(remove_non_english(text_new3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:18.670749Z",
     "start_time": "2019-11-14T20:05:18.667050Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_one_and_two_letter(text):\n",
    "    \n",
    "    text = ' '.join( [w for w in text.split() if len(w)>2] )\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:20.702395Z",
     "start_time": "2019-11-14T20:05:18.795222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process data using Natural Language Processing techniques: clean, remove stop words and lemmatize.\n",
    "# Nice way of processing the dataframe on the spot with functions\n",
    "\n",
    "df_all['clean_tweet'] = df_all['raw tweet'].apply(clean_tweets)\n",
    "df_all['clean_tweet'] = df_all['clean_tweet'].apply(remove_stopwords)\n",
    "df_all['clean_tweet'] = df_all['clean_tweet'].apply(lemmatize)\n",
    "df_all['clean_tweet'] = df_all['clean_tweet'].apply(remove_one_and_two_letter)\n",
    "df_all['clean_tweet'] = df_all['clean_tweet'].apply(remove_stopwords)\n",
    "df_all['clean_tweet'] = df_all['clean_tweet'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:22.304928Z",
     "start_time": "2019-11-14T20:05:22.301881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5159\n"
     ]
    }
   ],
   "source": [
    "# First pass of cleaning tweets\n",
    "print(len(df_all['clean_tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:22.928390Z",
     "start_time": "2019-11-14T20:05:22.920806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5159, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df_all.dropna()\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:24.724291Z",
     "start_time": "2019-11-14T20:05:24.721890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start removing non-english words\n",
    "# This will take a while to complete\n",
    "\n",
    "#clean_tweet2 = []\n",
    "\n",
    "#print('Cleaning and parsing the tweets...\\n')\n",
    "\n",
    "#update_freq = 100\n",
    "\n",
    "#total_tweets = len(df_all['clean_tweet'])\n",
    "\n",
    "#for i in range(0,len(df_all['clean_tweet'])):\n",
    "#    if( (i+1)%update_freq == 0 ):\n",
    "#        print('Tweets %d of %d has been processed' % ( i+1, total_tweets ) )                                                                   \n",
    "#    clean_tweet2.append(remove_non_english(df_all['clean_tweet'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:24.860269Z",
     "start_time": "2019-11-14T20:05:24.857937Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(clean_tweet2[667])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:25.153571Z",
     "start_time": "2019-11-14T20:05:25.013317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append to df_all\n",
    "#df_all['clean_tweet_v2'] = clean_tweet2\n",
    "\n",
    "#df_all\n",
    "\n",
    "# Now write out to csv file\n",
    "df_all.to_csv('./data/pulled_all_delta_airline_tweets_clean_subset_secs.csv',encoding='utf-8-sig')\n",
    "\n",
    "#df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T20:05:25.159426Z",
     "start_time": "2019-11-14T20:05:25.156636Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_temp = pd.read_csv('./data/pulled_all_airline_tweets_clean_final.csv',encoding ='utf-8-sig')\n",
    "\n",
    "#df_temp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-14T00:52:27.204238Z",
     "start_time": "2019-11-14T00:52:27.201773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set X matrix\n",
    "X = df_all['clean_tweet']\n",
    "#X = df_all['clean_tweet_v2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count-Vectorizer on entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is to look at the brute force vectorization feature output. I would like to know which misspelled words were missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:25:13.421022Z",
     "start_time": "2019-11-12T04:25:12.706279Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english',binary=False)\n",
    "\n",
    "X_cvec = cvec.fit_transform(X)\n",
    "\n",
    "df_cvec = pd.DataFrame(X_cvec.toarray(), columns=cvec.get_feature_names())\n",
    "\n",
    "df_cvec.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:25:16.152488Z",
     "start_time": "2019-11-12T04:25:16.146659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "# In this case I do not have a target as of yet hence Unsupervised Learning\n",
    "X_train, X_test = train_test_split(X, test_size=sample_test_size, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:55.549150Z",
     "start_time": "2019-11-12T04:30:52.154397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start with default options\n",
    "\n",
    "min_df_input = 8\n",
    "max_df_input = 0.9\n",
    "\n",
    "cv1 = CountVectorizer(stop_words='english',min_df=min_df_input,max_df=max_df_input)\n",
    "X_train_cv1 = cv1.fit_transform(X_train)\n",
    "X_test_cv1  = cv1.transform(X_test)\n",
    "\n",
    "# Bigrams\n",
    "cv2 = CountVectorizer(ngram_range=(1,2), stop_words='english',min_df=min_df_input,max_df=max_df_input)\n",
    "X_train_cv2 = cv2.fit_transform(X_train)\n",
    "X_test_cv2  = cv2.transform(X_test)\n",
    "\n",
    "# Trigrams\n",
    "cv3 = CountVectorizer(ngram_range=(1,3), stop_words='english',min_df=min_df_input,max_df=max_df_input)\n",
    "X_train_cv3 = cv3.fit_transform(X_train)\n",
    "X_test_cv3  = cv3.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:55.797073Z",
     "start_time": "2019-11-12T04:30:55.550953Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cv1 = pd.DataFrame(X_train_cv1.toarray(), columns=cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:55.811016Z",
     "start_time": "2019-11-12T04:30:55.799248Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cv1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:55.814613Z",
     "start_time": "2019-11-12T04:30:55.813021Z"
    }
   },
   "outputs": [],
   "source": [
    "#for col in data.columns: \n",
    "#    print(col) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:59.083428Z",
     "start_time": "2019-11-12T04:30:55.816155Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a stab at it\n",
    "# Default vectorizer options\n",
    "\n",
    "min_df_input = 10\n",
    "max_df_input = 0.90\n",
    "\n",
    "tfidf1         = TfidfVectorizer(stop_words='english',min_df=min_df_input,max_df=max_df_input)\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X_train)\n",
    "X_test_tfidf1  = tfidf1.transform(X_test)\n",
    "\n",
    "# Bi-grams\n",
    "tfidf2         = TfidfVectorizer(ngram_range=(1,2), stop_words='english',min_df=min_df_input,max_df=max_df_input)\n",
    "X_train_tfidf2 = tfidf2.fit_transform(X_train)\n",
    "X_test_tfidf2  = tfidf2.transform(X_test)\n",
    "\n",
    "# Tri-grams\n",
    "tfidf3         = TfidfVectorizer(ngram_range=(1,3), stop_words='english',min_df=min_df_input,max_df=max_df_input)\n",
    "X_train_tfidf3 = tfidf3.fit_transform(X_train)\n",
    "X_test_tfidf3  = tfidf3.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:59.363819Z",
     "start_time": "2019-11-12T04:30:59.085169Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tfidf1 = pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names()).head()\n",
    "\n",
    "df_tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:59.367061Z",
     "start_time": "2019-11-12T04:30:59.365297Z"
    }
   },
   "outputs": [],
   "source": [
    "#for col in data.columns: \n",
    "#    print(col) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:59.677805Z",
     "start_time": "2019-11-12T04:30:59.369920Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:30:59.987353Z",
     "start_time": "2019-11-12T04:30:59.679796Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_tfidf2.toarray(), columns=tfidf2.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:00.300322Z",
     "start_time": "2019-11-12T04:30:59.988884Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_tfidf3.toarray(), columns=tfidf3.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:00.680211Z",
     "start_time": "2019-11-12T04:31:00.301746Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_tfidf3.toarray(), columns=tfidf3.get_feature_names()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have more columns than rows. We definitely need to reduce the dimensionality of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:00.684168Z",
     "start_time": "2019-11-12T04:31:00.682061Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:00.688321Z",
     "start_time": "2019-11-12T04:31:00.686102Z"
    }
   },
   "outputs": [],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Singular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "\n",
    "#s_num_topics = 2\n",
    "#e_num_topics = s_num_topics+1\n",
    "\n",
    "#for i in range(s_num_topics,e_num_topics):\n",
    "    \n",
    "#    lsa = TruncatedSVD(i,n_iter=100)\n",
    "    \n",
    "#    X_train_cv1_lsa_topic = lsa.fit_transform(X_train_cv1)\n",
    "#    X_train_cv2_lsa_topic = lsa.fit_transform(X_train_cv2)\n",
    "#    X_train_cv3_lsa_topic = lsa.fit_transform(X_train_cv3)\n",
    "    \n",
    "#    X_train_tfidf1_lsa_topic = lsa.fit_transform(X_train_tfidf1)\n",
    "#    X_train_tfidf2_lsa_topic = lsa.fit_transform(X_train_tfidf2)\n",
    "#    X_train_tfidf3_lsa_topic = lsa.fit_transform(X_train_tfidf3)\n",
    "    \n",
    "#    lsa.explained_variance_ratio_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:02.153364Z",
     "start_time": "2019-11-12T04:31:00.690660Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 7\n",
    "\n",
    "lsa1 = TruncatedSVD(num_topics,n_iter=100, random_state=SEED)\n",
    "\n",
    "X_train_tfidf1_lsa1_topic = lsa1.fit_transform(X_train_tfidf1)\n",
    "\n",
    "lsa1.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:03.755685Z",
     "start_time": "2019-11-12T04:31:03.753847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bi-grams\n",
    "#lsa2 = TruncatedSVD(num_topics,n_iter=100)\n",
    "\n",
    "#X_train_tfidf2_lsa2_topic = lsa2.fit_transform(X_train_tfidf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:06.072536Z",
     "start_time": "2019-11-12T04:31:06.070481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tri-grams\n",
    "#lsa3 = TruncatedSVD(num_topics,n_iter=100)\n",
    "\n",
    "#X_train_tfidf3_lsa3_topic = lsa3.fit_transform(X_train_tfidf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:06.232540Z",
     "start_time": "2019-11-12T04:31:06.227947Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is from the lecture notebook (Topic modeling with LSA and NMF)\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix+1)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:06.737112Z",
     "start_time": "2019-11-12T04:31:06.730370Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(lsa1, tfidf1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T22:31:43.771654Z",
     "start_time": "2019-11-11T22:30:55.386349Z"
    }
   },
   "outputs": [],
   "source": [
    "# cosine similarity of training vect, LSA\n",
    "CS1 = cosine_similarity(X_train_tfidf1_lsa1_topic).round(3)\n",
    "print(CS1)\n",
    "print(CS1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T22:30:48.032842Z",
     "start_time": "2019-11-11T22:30:48.028454Z"
    }
   },
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "cosine_similarity((X_train_tfidf1_lsa1_topic[0], X_train_tfidf1_lsa1_topic[1], \n",
    "                   X_train_tfidf1_lsa1_topic[2], X_train_tfidf1_lsa1_topic[3],\n",
    "                   X_train_tfidf1_lsa1_topic[4], X_train_tfidf1_lsa1_topic[5])).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:12:53.691851Z",
     "start_time": "2019-11-11T04:12:53.684652Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_similarity((X_train_tfidf1_lsa1_topic[0], X_train_tfidf1_lsa1_topic[3])).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:43.953233Z",
     "start_time": "2019-11-12T04:31:43.951267Z"
    }
   },
   "outputs": [],
   "source": [
    "#Vt = pd.DataFrame(X_train_tfidf1_lsa1_topic.round(5),\n",
    "#                  index = X_train,\n",
    "#                  columns = [\"component_1\",\"component_2\",\"component_3\",\"component_4\",\"component_5\"])\n",
    "#Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:07:57.752540Z",
     "start_time": "2019-11-11T04:07:57.744555Z"
    }
   },
   "outputs": [],
   "source": [
    "#display_topics(lsa2, tfidf2.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:07:58.621533Z",
     "start_time": "2019-11-11T04:07:58.619522Z"
    }
   },
   "outputs": [],
   "source": [
    "#display_topics(lsa3, tfidf3.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T02:23:18.944366Z",
     "start_time": "2019-11-11T02:23:18.942252Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/\n",
    "#for i, comp in enumerate(lsa1.components_):\n",
    "#    terms_comp = zip(tfidf1.get_feature_names(), comp)\n",
    "#    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "#    print(\"Topic \"+str(i)+\": \")\n",
    "#    for t in sorted_terms:\n",
    "#        print(t[0])\n",
    "#        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:49.800738Z",
     "start_time": "2019-11-12T04:31:49.306003Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_topic = 7\n",
    "\n",
    "nmf1 = NMF(nmf_topic, random_state=SEED)\n",
    "X_train_tfidf1_nmf1_topic = nmf1.fit_transform(X_train_tfidf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:49.809199Z",
     "start_time": "2019-11-12T04:31:49.802660Z"
    }
   },
   "outputs": [],
   "source": [
    "# NMF with\n",
    "display_topics(nmf1, tfidf1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:31:50.180931Z",
     "start_time": "2019-11-12T04:31:50.172593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Topics from LSA\n",
    "display_topics(lsa1, tfidf1.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:54:21.108412Z",
     "start_time": "2019-11-12T05:54:20.998723Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:54:46.310378Z",
     "start_time": "2019-11-12T05:54:21.839913Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tweak the two parameters below\n",
    "lda_topics = 5\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda1 = LDA(n_components=lda_topics, n_jobs=-1, random_state=SEED)\n",
    "X_train_tfidf1_lda1_topic = lda1.fit_transform(X_train_tfidf1)\n",
    "\n",
    "lda2 = LDA(n_components=lda_topics, n_jobs=-1, random_state=SEED)\n",
    "X_train_tfidf2_lda2_topic = lda2.fit_transform(X_train_tfidf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:54:46.319957Z",
     "start_time": "2019-11-12T05:54:46.312723Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(lda1, tfidf1.get_feature_names(), number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:54:46.331448Z",
     "start_time": "2019-11-12T05:54:46.322415Z"
    }
   },
   "outputs": [],
   "source": [
    "display_topics(lda2, tfidf2.get_feature_names(), number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:54:46.335821Z",
     "start_time": "2019-11-12T05:54:46.333386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let plot using pLDAvis\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:55:29.597025Z",
     "start_time": "2019-11-12T05:54:50.998068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uni-grams\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_sk = pyLDAvis.sklearn.prepare(lda1, X_train_tfidf1, tfidf1)\n",
    "\n",
    "vis_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T05:56:34.234560Z",
     "start_time": "2019-11-12T05:55:29.598384Z"
    }
   },
   "outputs": [],
   "source": [
    "# bi-grams\n",
    "vis_sk2 = pyLDAvis.sklearn.prepare(lda2, X_train_tfidf2, tfidf2)\n",
    "\n",
    "vis_sk2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA w/o pyLDAvis (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:29:40.199209Z",
     "start_time": "2019-11-12T00:29:40.034635Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "#import gensim.corpora as corpora, models, similarities, matutils\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:29:40.203078Z",
     "start_time": "2019-11-12T00:29:40.200752Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_word = X_train_tfidf1.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:29:40.251678Z",
     "start_time": "2019-11-12T00:29:40.249632Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:29:40.378643Z",
     "start_time": "2019-11-12T00:29:40.375679Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in tfidf1.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:10.471742Z",
     "start_time": "2019-11-12T00:29:41.090608Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_wo_vis = models.LdaModel(corpus=corpus, num_topics=7, id2word=id2word, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:10.477695Z",
     "start_time": "2019-11-12T00:30:10.473578Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_wo_vis.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:10.483250Z",
     "start_time": "2019-11-12T00:30:10.479858Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_corpus = lda_wo_vis[corpus]\n",
    "lda_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA w/ pyLDAvis (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:10.571656Z",
     "start_time": "2019-11-12T00:30:10.485389Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:13.473867Z",
     "start_time": "2019-11-12T00:30:10.573106Z"
    }
   },
   "outputs": [],
   "source": [
    "# borrowed from class notebook\n",
    "#doc_word = X_train_cv1.transpose()\n",
    "# Tokenize text\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    return (text)\n",
    "\n",
    "text_data = []\n",
    "\n",
    "for line in X_train:\n",
    "    tokens = tokenize_text(line)\n",
    "    text_data.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:13.985852Z",
     "start_time": "2019-11-12T00:30:13.475520Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:14.252446Z",
     "start_time": "2019-11-12T00:30:13.987589Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:14.257112Z",
     "start_time": "2019-11-12T00:30:14.255366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "#doc_word = count_vectorizer.transform(ng_train.data).transpose()\n",
    "#corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:30:14.260456Z",
     "start_time": "2019-11-12T00:30:14.258890Z"
    }
   },
   "outputs": [],
   "source": [
    "#id2word = dict((v, k) for k, v in cv1.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:46:28.757090Z",
     "start_time": "2019-11-12T00:45:21.578018Z"
    }
   },
   "outputs": [],
   "source": [
    "#lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=5)\n",
    "lda_model = gensim.models.ldamulticore.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=7, \n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=5,\n",
    "                                            alpha='auto',\n",
    "                                            random_state=SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:46:32.825055Z",
     "start_time": "2019-11-12T00:46:32.814241Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:46:37.079252Z",
     "start_time": "2019-11-12T00:46:37.077418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the docs from the word space to the topic space (like \"transform\" in sklearn)\n",
    "#lda_corpus = lda_model[corpus]\n",
    "#lda_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:46:37.257776Z",
     "start_time": "2019-11-12T00:46:37.255842Z"
    }
   },
   "outputs": [],
   "source": [
    "#X_train_tfidf1_lda2_topic = lda_model.fit(X_train_tfidf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:50:03.568540Z",
     "start_time": "2019-11-12T00:46:38.796103Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:34:53.969501Z",
     "start_time": "2019-11-12T00:34:48.293784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus)) \n",
    "\n",
    "# Compute Coherence Score\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=X_train, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means with LSA (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T04:24:32.586010Z",
     "start_time": "2019-11-12T04:24:32.328808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Focus only on tf-idf\n",
    "s_num_cluster = 2\n",
    "e_num_cluster = 30\n",
    "\n",
    "cluster_range = range(s_num_cluster,e_num_cluster+1)\n",
    "\n",
    "inertia_vals1 = []\n",
    "inertia_vals2 = []\n",
    "inertia_vals3 = []\n",
    "\n",
    "for i in cluster_range:\n",
    "    \n",
    "    # n_init, number of times the K-mean algorithm will run\n",
    "    km1 = KMeans(n_clusters = i, random_state = SEED, n_jobs = -1) \n",
    "    #km2 = KMeans(n_clusters = i, random_state = SEED, n_jobs = -1)\n",
    "    #km3 = KMeans(n_clusters = i, random_state = SEED, n_jobs = -1)\n",
    "    \n",
    "    km_fit1 = km1.fit(X_train_tfidf1_lsa1_topic)\n",
    "    #km_fit2 = km2.fit(X_train_tfidf2_lsa2_topic)\n",
    "    #km_fit3 = km3.fit(X_train_tfidf3_lsa3_topic)\n",
    "    \n",
    "    inertia_vals1.append(km_fit1.inertia_)\n",
    "    #inertia_vals2.append(km_fit2.inertia_)\n",
    "    #inertia_vals3.append(km_fit3.inertia_)\n",
    "    \n",
    "    #print('Status: ',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:12:52.769950Z",
     "start_time": "2019-11-12T00:12:52.533456Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=12)\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.plot(cluster_range,inertia_vals1,linewidth=2.5,label='Uni-gram');\n",
    "#plt.plot(cluster_range,inertia_vals2,linewidth=2.5,label='Bi-gram');\n",
    "#plt.plot(cluster_range,inertia_vals3,linewidth=2.5,label='Tri-gram');\n",
    "plt.xlabel('Number of clusters, k');\n",
    "plt.ylabel('Inertia from KMeans');\n",
    "#plt.grid();\n",
    "plt.title('Inertia vs. Number of Clusters (SVD). Num Topics: '+ str(num_topics));\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('kmeans_inertia_tfidf.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:13:36.482495Z",
     "start_time": "2019-11-12T00:13:36.217530Z"
    }
   },
   "outputs": [],
   "source": [
    "# For two topics, let's pick k = 5 for uni-grams and k = 5 for bi and trigrams\n",
    "\n",
    "# Uni-gram\n",
    "num_clusters = 5\n",
    "\n",
    "km1f      = KMeans(n_clusters = num_clusters, n_jobs = -1, random_state=SEED)\n",
    "km_fit1f  = km1f.fit(X_train_tfidf1_lsa1_topic)\n",
    "\n",
    "#print(terms1)\n",
    "\n",
    "#clusters_1 = km1f.labels_.tolist()\n",
    "\n",
    "#print(clusters_1)\n",
    "\n",
    "#print(order_centroids1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:18:58.440709Z",
     "start_time": "2019-11-12T00:18:58.433403Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
    "def display_cluster(X, km, num_topics, num_clust):\n",
    "    color = 'brgcmyk'\n",
    "    alpha = 0.5\n",
    "    s = 20\n",
    "    \n",
    "    plt.title('k-Means, Topics:'+str(num_topics) + ' Clusters:' + str(num_clust))\n",
    "    \n",
    "    if num_clust == 0:\n",
    "        plt.scatter(X[:,0],X[:,1],c = color[0],alpha = alpha,s = s)\n",
    "    else:\n",
    "        for i in range(num_clust):\n",
    "            plt.scatter(X[km.labels_==i,0],X[km.labels_==i,1],c = color[i],alpha = alpha,s=s)\n",
    "            plt.scatter(km.cluster_centers_[i][0],km.cluster_centers_[i][1],c = color[i], marker = 'x', s = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:18:58.970367Z",
     "start_time": "2019-11-12T00:18:58.442898Z"
    }
   },
   "outputs": [],
   "source": [
    "display_cluster(X_train_tfidf1_lsa1_topic,km1f,num_topics,num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T00:19:58.452879Z",
     "start_time": "2019-11-12T00:18:58.972880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot all clusters\n",
    "\n",
    "max_num_clusters = 7\n",
    "\n",
    "for i in range(2,max_num_clusters+1):\n",
    "    \n",
    "    km1f      = KMeans(n_clusters = i, n_jobs = -1, random_state=SEED)\n",
    "    km_fit1f  = km1f.fit(X_train_tfidf1_lsa1_topic)\n",
    "    \n",
    "    display_cluster(X_train_tfidf1_lsa1_topic,km1f,5,i)\n",
    "    \n",
    "    image_file_name = 'kmeans_tfidf_topic_'+str(num_topics)+'_'+'cluster_'+str(i)+'.svg'\n",
    "\n",
    "    plt.savefig(image_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T00:18:18.442993Z",
     "start_time": "2019-11-11T00:18:18.440856Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(len(terms1))\n",
    "\n",
    "#print(\"Top terms per cluster:\")\n",
    "\n",
    "#order_centroids1 = km1f.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "#terms1 = tfidf1.get_feature_names()\n",
    "\n",
    "#for i in range(n_clust):\n",
    "#    print(\"Cluster %d:\" % i),\n",
    "#    for ind in order_centroids1[i, :50]:\n",
    "#        print(' %s' % terms1[ind]),\n",
    "\n",
    "#print(terms1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:43:24.709665Z",
     "start_time": "2019-11-11T04:43:20.773344Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create one big looping over number of topics and clusters\n",
    "# Number topics is from 2 to 10\n",
    "# Number of clusters is from 2 to 7\n",
    "\n",
    "n_topics   = 2\n",
    "n_clusters = 2\n",
    "\n",
    "for i_topic in range(2,n_topics+1):\n",
    "    for i_clust in range(2,n_clusters+1):\n",
    "        \n",
    "        lsa1 = TruncatedSVD(i_topic,n_iter=100)\n",
    "\n",
    "        X_train_tfidf1_lsa1_topic = lsa1.fit_transform(X_train_tfidf1)\n",
    "    \n",
    "        km1f      = KMeans(n_clusters = i_clust, n_jobs = -1, random_state=SEED)\n",
    "    \n",
    "        km_fit1f  = km1f.fit(X_train_tfidf1_lsa1_topic)\n",
    "    \n",
    "        display_cluster(X_train_tfidf1_lsa1_topic,km1f,i_topic,i_clust)\n",
    "    \n",
    "        image_file_name = 'kmeans_tfidf_topic_'+str(i_topic)+'_'+'cluster_'+str(i_clust)+'.svg'\n",
    "\n",
    "        plt.savefig(image_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T20:10:58.404132Z",
     "start_time": "2019-11-10T20:10:58.352940Z"
    }
   },
   "source": [
    "#### K-Means with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:34:59.813136Z",
     "start_time": "2019-11-11T04:34:29.536026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Focus only on tf-idf\n",
    "s_num_cluster = 2\n",
    "e_num_cluster = 30\n",
    "\n",
    "cluster_range = range(s_num_cluster,e_num_cluster+1)\n",
    "\n",
    "inertia_vals_nmf1 = []\n",
    "\n",
    "for i in cluster_range:\n",
    "    \n",
    "    # n_init, number of times the K-mean algorithm will run\n",
    "    km1 = KMeans(n_clusters = i, random_state = SEED, n_jobs = -1) \n",
    "\n",
    "    km_fit_nmf1 = km1.fit(X_train_tfidf1_nmf1_topic)\n",
    "\n",
    "    inertia_vals_nmf1.append(km_fit_nmf1.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T05:29:47.756020Z",
     "start_time": "2019-11-11T05:29:47.414286Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=12)\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.plot(cluster_range,inertia_vals_nmf1,linewidth=2.5,label='Uni-gram, NMF');\n",
    "plt.xlabel('Number of clusters, k');\n",
    "plt.ylabel('Inertia from KMeans');\n",
    "#plt.grid();\n",
    "plt.title('Kmeans Inertia NMF. Num Topics: '+ str(num_topics));\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('kmeans_inertia_tfidf_nmf_topic3.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:39:54.963483Z",
     "start_time": "2019-11-11T04:39:54.252184Z"
    }
   },
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "\n",
    "km1n      = KMeans(n_clusters = num_clusters, n_jobs = -1, random_state=SEED)\n",
    "km_fit_nmf1  = km1n.fit(X_train_tfidf1_nmf1_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T04:40:48.483135Z",
     "start_time": "2019-11-11T04:40:47.962899Z"
    }
   },
   "outputs": [],
   "source": [
    "display_cluster(X_train_tfidf1_nmf1_topic,km1n,nmf_topic,num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means with LDA from Sci-kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:54:57.876874Z",
     "start_time": "2019-11-11T23:54:31.821595Z"
    }
   },
   "outputs": [],
   "source": [
    "# Focus only on tf-idf\n",
    "s_num_cluster = 2\n",
    "e_num_cluster = 30\n",
    "\n",
    "cluster_range = range(s_num_cluster,e_num_cluster+1)\n",
    "\n",
    "inertia_vals_lda1 = []\n",
    "\n",
    "for i in cluster_range:\n",
    "    \n",
    "    # n_init, number of times the K-mean algorithm will run\n",
    "    km1 = KMeans(n_clusters = i, random_state = SEED, n_jobs = -1) \n",
    "\n",
    "    km_fit_lda1 = km1.fit(X_train_tfidf1_lda1_topic)\n",
    "\n",
    "    inertia_vals_lda1.append(km_fit_lda1.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:54:58.188550Z",
     "start_time": "2019-11-11T23:54:57.879779Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc('font', size=12)\n",
    "plt.figure(figsize=(10,8));\n",
    "plt.plot(cluster_range,inertia_vals_lda1,linewidth=2.5,label='Uni-gram, LDA');\n",
    "plt.xlabel('Number of clusters, k');\n",
    "plt.ylabel('Inertia from KMeans');\n",
    "#plt.grid();\n",
    "plt.title('Kmeans Inertia LDA. Num Topics: '+ str(lda_topics));\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('kmeans_inertia_tfidf_nmf_topic5.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:54:58.286195Z",
     "start_time": "2019-11-11T23:54:58.190484Z"
    }
   },
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "\n",
    "km_lda1      = KMeans(n_clusters = num_clusters, n_jobs = -1, random_state=SEED)\n",
    "km_fit_lda1  = km_lda1.fit(X_train_tfidf1_lda1_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T23:54:58.918499Z",
     "start_time": "2019-11-11T23:54:58.289113Z"
    }
   },
   "outputs": [],
   "source": [
    "display_cluster(X_train_tfidf1_lda1_topic,km_lda1,lda_topics,num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "291.264px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
